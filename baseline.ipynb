{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline\n",
    "\n",
    "Creates paraphrases based on a word-for-word replacement with synonyms from the [CJVT Thesaurus of Modern Slovene](http://hdl.handle.net/11356/1166)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize, treebank\n",
    "import os\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:05:49.815539230Z",
     "start_time": "2023-05-16T20:05:49.282643209Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loop over entries: 100%|██████████| 105473/105473 [00:04<00:00, 25705.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all synonyms in the dictionary: 81297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dict_loc = \"data/thesaurus_dict.pkl\"\n",
    "\n",
    "if os.path.exists(dict_loc) and os.path.isdir(dict_loc):\n",
    "    with open(dict_loc, \"rb\") as f:\n",
    "        thesaurus = pkl.load(f)\n",
    "else:\n",
    "    with open(\"data/CJVT_Thesaurus-v1.0/CJVT_Thesaurus-v1.0.xml\") as f:\n",
    "        xml_doc = f.read()\n",
    "        soup = BeautifulSoup(xml_doc, 'xml')\n",
    "    thesaurus = dict()\n",
    "    for entry in tqdm(soup.find_all(\"entry\"), desc=\"loop over entries\"):\n",
    "        if entry.groups_core: # TODO: add some threshold based on score in candidate xml tags\n",
    "            candidate = entry.groups_core.group.find(\"candidate\")\n",
    "            if float(candidate[\"score\"]) >= 0.16:\n",
    "                thesaurus[str(entry.headword.string)] = str(candidate.s.string)\n",
    "    with open(dict_loc, \"wb\") as f:\n",
    "        pkl.dump(thesaurus, f)\n",
    "\n",
    "print(\"Number of all synonyms in the dictionary:\", len(thesaurus))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:06:19.321845773Z",
     "start_time": "2023-05-16T20:05:49.818088870Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data/4th_test.pkl\").df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:06:20.110418906Z",
     "start_time": "2023-05-16T20:06:19.318835919Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# optional quick check of dataset\n",
    "#for _, (c1, c2) in df.iterrows():\n",
    "#    print(c1, \"\\n\", c2)\n",
    "#    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:06:20.113639591Z",
     "start_time": "2023-05-16T20:06:20.111685197Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIG: Za razvojno perspektivo učenja jezika (semantike, skladnje in pragmatike) ni pomembno le učenje slovničnih pravil, ampak so pomembne vse vrste višjih duševnih procesov, še posebej učenje konceptov oz. pojmov.\n",
      "BACKT: Za razvojno perspektivo učenja jezikov (semantika, sintaksa in pragmatika) ni pomembno le, da se naučimo slovničnih pravil, temveč so pomembne vse vrste višjih miselnih procesov, zlasti učenje konceptov.\n",
      "BASE: Za razvojno perspektivo učenja jezika (semantike, skladnje in pragmatike) ni važno samo uk slovničnih pravil, ampak so pomembne vse krogi višjih duševnih procesov, še posebej uk konceptov oz . pojmov.\n",
      "\n",
      "ORIG: Velike temperaturne obremenitve in visoki tlaki so zahtevali svoj davek in taki motorji so bili precej občutljivi in dokaj zahtevni za vzdrževanje. Velik problem je bila tudi \"turbo luknja\" oziroma pomanjkanje moči pri nižjih motornih vrtljajih. Kot kaže so pri Volkswagnu našli rešitev za vsakdanje potrebe. Združili so svojo turbo tehnologijo in neposredni vbrizg goriva in naredili zelo učinkovit turbobencinski motor. Iz 1,4-litrskega motorja so tokrat iztisnili »le« 140 KM namesto 170, kot pri Golfu GT. Gre namreč za enak motor (EA 111), enako tehnologijo, le nekaj manj moči ima.\n",
      "BACKT: Velike toplotne obremenitve in visoki pritiski so vzeli svoj davek, in takšni motorji so bili zelo občutljivi in težko vzdrževati. Še en velik problem je bil \"turbo luknja\" ali pomanjkanje moči pri nižjih hitrostih motorja. Volkswagen se zdi, da so našli rešitev za vsakdanje potrebe. Ti v kombinaciji svoje turbo tehnologijo in neposredno vbrizgavanje goriva, da bi zelo učinkovit turbopolnilnik motor. Tokrat, 1,4-litrski motor je bil iztisnjen \"samo\" 140 KM namesto 170, kot v primeru Golf GT. To je isti motor (EA 111), ista tehnologija, le nekoliko manj moči.\n",
      "BASE: Velike temperaturne obremenitve in visoki tlaki so zahtevali svoj dajatev in taki motorji so bili precej občutljivi in precej zahtevni za vzdrževanje . Velik težava je bila tudi \"turbo luknja\" oziroma pomanjkanje znati pri nižjih motornih vrtljajih . Kot da so pri Volkswagnu našli rešitev za vsakdanje nameni . Združili so svojo turbo tehnologijo in neposredni vbrizg goriva in naredili zelo učinkovit turbobencinski motor . Iz 1,4-litrskega motorja so tokrat iztisnili » samo « 140 KM za 170, kot pri Golfu GT . Gre in sicer za enak motor (EA 111), enako tehnologijo, samo nekaj minus znati ima.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, (orig, backt) in list(df.iterrows())[:2]: # TODO: remove list(...)[:2] to get all paraphrases\n",
    "    new_stt = []\n",
    "    for token in word_tokenize(orig):\n",
    "        if token in thesaurus:\n",
    "            new_stt.append(thesaurus[token])\n",
    "        else:\n",
    "            new_stt.append(token)\n",
    "    print(f\"ORIG: {orig}\\nBACKT: {backt}\\nBASE: {treebank.TreebankWordDetokenizer().detokenize(new_stt)}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-16T20:06:20.604266938Z",
     "start_time": "2023-05-16T20:06:20.116851361Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
