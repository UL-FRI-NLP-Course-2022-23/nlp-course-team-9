# TODO

- ročno evalvirat podatke iz back-translationa
- na koncu pognati na testnih podatkih
- izbrati kako velik model uporabiti ali pa kakšnega drugega (https://www.kdnuggets.com/2023/02/simple-nlp-pipelines-huggingface-transformers.html)
- mogoče mal povečat learning rate pa dodat nek weight_decay (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)
